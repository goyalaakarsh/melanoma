{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a71dc09e",
   "metadata": {},
   "source": [
    "# Advanced Melanoma Segmentation with Self-Supervised Learning + Mask R-CNN\n",
    "\n",
    "This notebook implements a state-of-the-art deep learning pipeline for melanoma lesion segmentation using:\n",
    "\n",
    "1. **Self-Supervised Learning (SSL)**: Pre-train a Vision Transformer (ViT) using Masked Autoencoder (MAE) approach\n",
    "2. **Mask R-CNN Fine-tuning**: Use the SSL backbone with Detectron2 for instance segmentation\n",
    "3. **Advanced Augmentation**: Albumentations pipeline for robust training\n",
    "\n",
    "## Pipeline Overview\n",
    "- Phase 1: Environment Setup & Dependencies\n",
    "- Phase 2: SSL Pre-training with MAE\n",
    "- Phase 3: Mask R-CNN Fine-tuning with Custom ViT Backbone\n",
    "- Phase 4: Evaluation & Model Export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0397c995",
   "metadata": {},
   "source": [
    "## Phase 1: Environment Setup & Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27fdd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Detectron2 imports\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultTrainer, default_argument_parser, default_setup\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data import build_detection_train_loader, build_detection_test_loader\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "# Hugging Face Transformers for MAE\n",
    "from transformers import ViTMAEForPreTraining, ViTMAEConfig, ViTFeatureExtractor\n",
    "\n",
    "# Additional libraries\n",
    "import timm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Setup logging\n",
    "setup_logger()\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Detectron2 version: {detectron2.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec5223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Paths\n",
    "class Config:\n",
    "    # Data paths\n",
    "    DATA_ROOT = Path(\"data\")\n",
    "    UNLABELED_IMAGES_PATH = DATA_ROOT / \"images\"  # For SSL pre-training\n",
    "    TRAIN_IMAGES_PATH = DATA_ROOT / \"train\" / \"images\"  # For supervised training\n",
    "    TRAIN_MASKS_PATH = DATA_ROOT / \"train\" / \"masks\"\n",
    "    VAL_IMAGES_PATH = DATA_ROOT / \"val\" / \"images\"\n",
    "    VAL_MASKS_PATH = DATA_ROOT / \"val\" / \"masks\"\n",
    "    \n",
    "    # Model paths\n",
    "    MODELS_PATH = Path(\"models\")\n",
    "    SSL_BACKBONE_PATH = MODELS_PATH / \"ssl_vit_backbone.pth\"\n",
    "    FINAL_MODEL_PATH = MODELS_PATH / \"final_lesion_segmenter.pth\"\n",
    "    CONFIG_PATH = MODELS_PATH / \"config.yaml\"\n",
    "    \n",
    "    # Training parameters\n",
    "    IMAGE_SIZE = (224, 224)  # For ViT\n",
    "    BATCH_SIZE_SSL = 32  # For MAE pre-training\n",
    "    BATCH_SIZE_DETECTRON = 4  # For Mask R-CNN training\n",
    "    NUM_EPOCHS_SSL = 100\n",
    "    LEARNING_RATE_SSL = 1.5e-4\n",
    "    LEARNING_RATE_DETECTRON = 1e-4\n",
    "    MAX_ITER_DETECTRON = 5000\n",
    "    \n",
    "    # MAE parameters\n",
    "    MASK_RATIO = 0.75  # 75% of patches masked\n",
    "    PATCH_SIZE = 16\n",
    "    \n",
    "    # Device\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Create directories if they don't exist\n",
    "        self.MODELS_PATH.mkdir(exist_ok=True)\n",
    "        self.DATA_ROOT.mkdir(exist_ok=True)\n",
    "        for path in [self.UNLABELED_IMAGES_PATH, self.TRAIN_IMAGES_PATH, \n",
    "                     self.TRAIN_MASKS_PATH, self.VAL_IMAGES_PATH, self.VAL_MASKS_PATH]:\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = Config()\n",
    "print(f\"Configuration loaded. Device: {config.DEVICE}\")\n",
    "print(f\"Models will be saved to: {config.MODELS_PATH}\")\n",
    "print(f\"Data should be placed in: {config.DATA_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d3e37",
   "metadata": {},
   "source": [
    "## Phase 2: Self-Supervised Learning with MAE\n",
    "\n",
    "### Dataset for Unlabeled Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a4c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlabeledSkinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for unlabeled skin images used in SSL pre-training.\n",
    "    Loads all images from a directory and applies standard ViT transforms.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, image_size=(224, 224)):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Find all image files\n",
    "        self.image_paths = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']:\n",
    "            self.image_paths.extend(list(self.data_dir.rglob(ext)))\n",
    "            self.image_paths.extend(list(self.data_dir.rglob(ext.upper())))\n",
    "        \n",
    "        if not self.image_paths:\n",
    "            raise ValueError(f\"No images found in {data_dir}\")\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} images for SSL training\")\n",
    "        \n",
    "        # ViT preprocessing transforms\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Apply transforms\n",
    "            image_tensor = self.transform(image)\n",
    "            \n",
    "            return image_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            return torch.zeros(3, *self.image_size)\n",
    "\n",
    "# Example usage (will be used later when data is available)\n",
    "# ssl_dataset = UnlabeledSkinDataset(config.UNLABELED_IMAGES_PATH)\n",
    "# ssl_dataloader = DataLoader(ssl_dataset, batch_size=config.BATCH_SIZE_SSL, \n",
    "#                           shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"UnlabeledSkinDataset class defined successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ccb53",
   "metadata": {},
   "source": [
    "### MAE Pre-training Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aead1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAETrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for Masked Autoencoder (MAE) pre-training.\n",
    "    Uses HuggingFace ViTMAEForPreTraining for efficient implementation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = config.DEVICE\n",
    "        \n",
    "        # Initialize MAE model\n",
    "        self.model = ViTMAEForPreTraining.from_pretrained('facebook/vit-mae-base')\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Optimizer and scheduler\n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=config.LEARNING_RATE_SSL, \n",
    "                                   weight_decay=0.05)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=config.NUM_EPOCHS_SSL\n",
    "        )\n",
    "        \n",
    "        # Loss tracking\n",
    "        self.train_losses = []\n",
    "        \n",
    "    def train_epoch(self, dataloader, epoch):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch_idx, images in enumerate(dataloader):\n",
    "            images = images.to(self.device)\n",
    "            \n",
    "            # Forward pass through MAE\n",
    "            outputs = self.model(images)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Backward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Batch {batch_idx}/{len(dataloader)}, '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        self.train_losses.append(avg_loss)\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    def visualize_reconstruction(self, dataloader, epoch, num_samples=4):\n",
    "        \"\"\"Visualize MAE reconstruction results\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get a batch of images\n",
    "            images = next(iter(dataloader))[:num_samples].to(self.device)\n",
    "            \n",
    "            # Get MAE outputs\n",
    "            outputs = self.model(images)\n",
    "            \n",
    "            # Denormalize images for visualization\n",
    "            mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(self.device)\n",
    "            std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(self.device)\n",
    "            \n",
    "            original = images * std + mean\n",
    "            reconstructed = outputs.logits * std + mean\n",
    "            \n",
    "            # Clip values to [0, 1]\n",
    "            original = torch.clamp(original, 0, 1)\n",
    "            reconstructed = torch.clamp(reconstructed, 0, 1)\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, axes = plt.subplots(2, num_samples, figsize=(15, 6))\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                # Original images\n",
    "                axes[0, i].imshow(original[i].cpu().permute(1, 2, 0))\n",
    "                axes[0, i].set_title(f'Original {i+1}')\n",
    "                axes[0, i].axis('off')\n",
    "                \n",
    "                # Reconstructed images\n",
    "                axes[1, i].imshow(reconstructed[i].cpu().permute(1, 2, 0))\n",
    "                axes[1, i].set_title(f'Reconstructed {i+1}')\n",
    "                axes[1, i].axis('off')\n",
    "            \n",
    "            plt.suptitle(f'MAE Reconstruction - Epoch {epoch}')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    def train(self, dataloader):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"Starting MAE pre-training for {self.config.NUM_EPOCHS_SSL} epochs...\")\n",
    "        \n",
    "        for epoch in range(self.config.NUM_EPOCHS_SSL):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config.NUM_EPOCHS_SSL}\")\n",
    "            \n",
    "            # Train for one epoch\n",
    "            avg_loss = self.train_epoch(dataloader, epoch+1)\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Visualize reconstructions every 20 epochs\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                self.visualize_reconstruction(dataloader, epoch+1)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        print(\"MAE pre-training completed!\")\n",
    "    \n",
    "    def save_backbone(self):\n",
    "        \"\"\"Save only the ViT encoder weights (backbone)\"\"\"\n",
    "        print(\"Saving SSL backbone weights...\")\n",
    "        \n",
    "        # Extract encoder weights\n",
    "        encoder_state_dict = {}\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not name.startswith('decoder'):  # Only save encoder weights\n",
    "                encoder_state_dict[name] = param.cpu()\n",
    "        \n",
    "        # Save backbone weights\n",
    "        torch.save(encoder_state_dict, self.config.SSL_BACKBONE_PATH)\n",
    "        \n",
    "        # Also save the full model config for reference\n",
    "        config_dict = {\n",
    "            'model_name': 'facebook/vit-mae-base',\n",
    "            'image_size': self.config.IMAGE_SIZE,\n",
    "            'patch_size': 16,\n",
    "            'hidden_size': 768,\n",
    "            'num_hidden_layers': 12,\n",
    "            'num_attention_heads': 12,\n",
    "            'intermediate_size': 3072,\n",
    "            'hidden_dropout_prob': 0.0,\n",
    "            'attention_probs_dropout_prob': 0.0,\n",
    "            'initializer_range': 0.02,\n",
    "            'layer_norm_eps': 1e-12,\n",
    "        }\n",
    "        \n",
    "        with open(self.config.MODELS_PATH / \"ssl_config.json\", 'w') as f:\n",
    "            json.dump(config_dict, f, indent=2)\n",
    "        \n",
    "        print(f\"SSL backbone saved to {self.config.SSL_BACKBONE_PATH}\")\n",
    "\n",
    "print(\"MAETrainer class defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac37f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSL Training Execution\n",
    "def run_ssl_training():\n",
    "    \"\"\"\n",
    "    Execute SSL pre-training with MAE.\n",
    "    This function will be called when unlabeled data is available.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if SSL backbone already exists\n",
    "    if config.SSL_BACKBONE_PATH.exists():\n",
    "        print(f\"SSL backbone already exists at {config.SSL_BACKBONE_PATH}\")\n",
    "        print(\"Skipping SSL training. To retrain, delete the existing file first.\")\n",
    "        return\n",
    "    \n",
    "    # Check if unlabeled data is available\n",
    "    if not config.UNLABELED_IMAGES_PATH.exists() or not any(config.UNLABELED_IMAGES_PATH.iterdir()):\n",
    "        print(f\"No unlabeled images found in {config.UNLABELED_IMAGES_PATH}\")\n",
    "        print(\"Please place unlabeled skin images in the data/images/ directory\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create dataset and dataloader\n",
    "        print(\"Creating unlabeled dataset...\")\n",
    "        ssl_dataset = UnlabeledSkinDataset(config.UNLABELED_IMAGES_PATH)\n",
    "        ssl_dataloader = DataLoader(\n",
    "            ssl_dataset, \n",
    "            batch_size=config.BATCH_SIZE_SSL, \n",
    "            shuffle=True, \n",
    "            num_workers=2,  # Reduced for Windows compatibility\n",
    "            pin_memory=True if torch.cuda.is_available() else False\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        print(\"Initializing MAE trainer...\")\n",
    "        trainer = MAETrainer(config)\n",
    "        \n",
    "        # Start training\n",
    "        trainer.train(ssl_dataloader)\n",
    "        \n",
    "        # Save backbone weights\n",
    "        trainer.save_backbone()\n",
    "        \n",
    "        # Plot training losses\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(trainer.train_losses)\n",
    "        plt.title('MAE Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"SSL pre-training completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during SSL training: {e}\")\n",
    "        print(\"Make sure you have unlabeled images in the data/images/ directory\")\n",
    "\n",
    "# Uncomment the line below to run SSL training when data is available\n",
    "# run_ssl_training()\n",
    "\n",
    "print(\"SSL training function defined. Call run_ssl_training() when data is ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84dbd58",
   "metadata": {},
   "source": [
    "## Phase 3: Mask R-CNN Fine-tuning with Custom ViT Backbone\n",
    "\n",
    "### Dataset for Labeled Images (Detectron2 Format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def3e1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melanoma_dicts(img_dir, mask_dir):\n",
    "    \"\"\"\n",
    "    Load dataset in Detectron2 format.\n",
    "    Converts binary masks to COCO format with bounding boxes and segmentation polygons.\n",
    "    \"\"\"\n",
    "    dataset_dicts = []\n",
    "    \n",
    "    # Get all image files\n",
    "    img_files = []\n",
    "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.tiff']:\n",
    "        img_files.extend(list(Path(img_dir).glob(ext)))\n",
    "        img_files.extend(list(Path(img_dir).glob(ext.upper())))\n",
    "    \n",
    "    for idx, img_path in enumerate(img_files):\n",
    "        record = {}\n",
    "        \n",
    "        # Image information\n",
    "        record[\"file_name\"] = str(img_path)\n",
    "        record[\"image_id\"] = idx\n",
    "        \n",
    "        # Load image to get dimensions\n",
    "        image = cv2.imread(str(img_path))\n",
    "        if image is None:\n",
    "            continue\n",
    "            \n",
    "        height, width = image.shape[:2]\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        \n",
    "        # Load corresponding mask\n",
    "        mask_path = Path(mask_dir) / (img_path.stem + \"_mask\" + img_path.suffix)\n",
    "        if not mask_path.exists():\n",
    "            # Try alternative naming conventions\n",
    "            for alt_ext in ['.png', '.jpg', '.jpeg']:\n",
    "                alt_mask_path = Path(mask_dir) / (img_path.stem + \"_mask\" + alt_ext)\n",
    "                if alt_mask_path.exists():\n",
    "                    mask_path = alt_mask_path\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Warning: No mask found for {img_path}\")\n",
    "                continue\n",
    "        \n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            print(f\"Warning: Could not load mask {mask_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Find contours in mask\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        objs = []\n",
    "        for contour in contours:\n",
    "            # Skip very small contours\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area < 100:  # Minimum area threshold\n",
    "                continue\n",
    "            \n",
    "            # Get bounding box\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            \n",
    "            # Convert contour to polygon format\n",
    "            polygon = contour.flatten().tolist()\n",
    "            \n",
    "            obj = {\n",
    "                \"bbox\": [x, y, x + w, y + h],  # [x1, y1, x2, y2] format\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [polygon],\n",
    "                \"category_id\": 0,  # Single class: lesion\n",
    "                \"area\": area\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        \n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    \n",
    "    print(f\"Loaded {len(dataset_dicts)} images from {img_dir}\")\n",
    "    return dataset_dicts\n",
    "\n",
    "def register_datasets():\n",
    "    \"\"\"Register datasets with Detectron2\"\"\"\n",
    "    \n",
    "    # Register training dataset\n",
    "    DatasetCatalog.register(\"melanoma_train\", \n",
    "                          lambda: get_melanoma_dicts(str(config.TRAIN_IMAGES_PATH), \n",
    "                                                   str(config.TRAIN_MASKS_PATH)))\n",
    "    MetadataCatalog.get(\"melanoma_train\").set(thing_classes=[\"lesion\"])\n",
    "    \n",
    "    # Register validation dataset\n",
    "    DatasetCatalog.register(\"melanoma_val\", \n",
    "                          lambda: get_melanoma_dicts(str(config.VAL_IMAGES_PATH), \n",
    "                                                   str(config.VAL_MASKS_PATH)))\n",
    "    MetadataCatalog.get(\"melanoma_val\").set(thing_classes=[\"lesion\"])\n",
    "    \n",
    "    print(\"Datasets registered with Detectron2\")\n",
    "\n",
    "# Register datasets\n",
    "register_datasets()\n",
    "\n",
    "# Test dataset loading\n",
    "try:\n",
    "    train_dicts = get_melanoma_dicts(str(config.TRAIN_IMAGES_PATH), str(config.TRAIN_MASKS_PATH))\n",
    "    val_dicts = get_melanoma_dicts(str(config.VAL_IMAGES_PATH), str(config.VAL_MASKS_PATH))\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dicts)}\")\n",
    "    print(f\"Validation samples: {len(val_dicts)}\")\n",
    "    \n",
    "    if train_dicts:\n",
    "        print(f\"Sample training image: {train_dicts[0]['file_name']}\")\n",
    "        print(f\"Number of annotations: {len(train_dicts[0]['annotations'])}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Dataset loading test failed: {e}\")\n",
    "    print(\"Make sure you have labeled images and masks in data/train/ and data/val/ directories\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f723d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7beb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Albumentations augmentation pipeline\n",
    "def get_augmentation_pipeline():\n",
    "    \"\"\"\n",
    "    Create robust augmentation pipeline for medical images.\n",
    "    Includes geometric, photometric, and elastic transformations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training augmentations\n",
    "    train_transform = A.Compose([\n",
    "        # Geometric transformations\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.1, \n",
    "            scale_limit=0.2, \n",
    "            rotate_limit=45, \n",
    "            p=0.7,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0\n",
    "        ),\n",
    "        A.RandomResizedCrop(\n",
    "            height=512, \n",
    "            width=512, \n",
    "            scale=(0.8, 1.0), \n",
    "            ratio=(0.8, 1.2), \n",
    "            p=0.5\n",
    "        ),\n",
    "        \n",
    "        # Photometric transformations\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2, \n",
    "            contrast_limit=0.2, \n",
    "            p=0.5\n",
    "        ),\n",
    "        A.HueSaturationValue(\n",
    "            hue_shift_limit=20, \n",
    "            sat_shift_limit=30, \n",
    "            val_shift_limit=20, \n",
    "            p=0.3\n",
    "        ),\n",
    "        A.RandomGamma(gamma_limit=(80, 120), p=0.3),\n",
    "        \n",
    "        # Advanced augmentations\n",
    "        A.ElasticTransform(\n",
    "            alpha=1, \n",
    "            sigma=50, \n",
    "            alpha_affine=50, \n",
    "            p=0.3,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0\n",
    "        ),\n",
    "        A.GridDistortion(\n",
    "            num_steps=5, \n",
    "            distort_limit=0.1, \n",
    "            p=0.2,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0\n",
    "        ),\n",
    "        A.OpticalDistortion(\n",
    "            distort_limit=0.1, \n",
    "            shift_limit=0.05, \n",
    "            p=0.2,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0\n",
    "        ),\n",
    "        \n",
    "        # Noise and blur\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.2),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.2),\n",
    "        A.MotionBlur(blur_limit=7, p=0.2),\n",
    "        \n",
    "        # Cutout-style augmentation\n",
    "        A.CoarseDropout(\n",
    "            max_holes=8, \n",
    "            max_height=32, \n",
    "            max_width=32, \n",
    "            min_holes=1,\n",
    "            min_height=8,\n",
    "            min_width=8,\n",
    "            fill_value=0,\n",
    "            p=0.3\n",
    "        ),\n",
    "        \n",
    "        # Normalization\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "    \n",
    "    # Validation transforms (minimal)\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(height=512, width=512),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], \n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "# Test augmentation pipeline\n",
    "def visualize_augmentations(image_path, mask_path, num_samples=4):\n",
    "    \"\"\"Visualize augmentation effects on a sample image\"\"\"\n",
    "    \n",
    "    # Load image and mask\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Get augmentation pipeline\n",
    "    train_transform, _ = get_augmentation_pipeline()\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(20, 8))\n",
    "    \n",
    "    # Original\n",
    "    axes[0, 0].imshow(image)\n",
    "    axes[0, 0].set_title('Original Image')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(mask, cmap='gray')\n",
    "    axes[1, 0].set_title('Original Mask')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    # Augmented samples\n",
    "    for i in range(1, num_samples):\n",
    "        try:\n",
    "            # Apply augmentation\n",
    "            augmented = train_transform(\n",
    "                image=image, \n",
    "                mask=mask,\n",
    "                bboxes=[[0, 0, image.shape[1], image.shape[0]]],  # Dummy bbox\n",
    "                class_labels=[0]  # Dummy class\n",
    "            )\n",
    "            \n",
    "            aug_image = augmented['image'].permute(1, 2, 0).numpy()\n",
    "            aug_mask = augmented['mask'].numpy()\n",
    "            \n",
    "            # Denormalize image for display\n",
    "            aug_image = aug_image * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "            aug_image = np.clip(aug_image, 0, 1)\n",
    "            \n",
    "            axes[0, i].imshow(aug_image)\n",
    "            axes[0, i].set_title(f'Augmented {i}')\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "            axes[1, i].imshow(aug_mask, cmap='gray')\n",
    "            axes[1, i].set_title(f'Augmented Mask {i}')\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in augmentation {i}: {e}\")\n",
    "            axes[0, i].text(0.5, 0.5, 'Error', ha='center', va='center')\n",
    "            axes[1, i].text(0.5, 0.5, 'Error', ha='center', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Augmentation pipeline defined successfully\")\n",
    "print(\"Use visualize_augmentations(image_path, mask_path) to test augmentations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df24bc04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d9e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fvcore.nn.weight_init as weight_init\n",
    "from detectron2.layers import Conv2d, ShapeSpec\n",
    "from detectron2.modeling.backbone import Backbone\n",
    "from detectron2.modeling.backbone.build import BACKBONE_REGISTRY\n",
    "\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Extract multi-scale features from ViT encoder.\n",
    "    Converts 1D patch tokens back to 2D feature maps at different scales.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vit_model, feature_layers=[3, 6, 9, 12]):\n",
    "        super().__init__()\n",
    "        self.vit_model = vit_model\n",
    "        self.feature_layers = feature_layers\n",
    "        self.patch_size = vit_model.config.patch_size\n",
    "        self.hidden_size = vit_model.config.hidden_size\n",
    "        \n",
    "        # Freeze ViT parameters\n",
    "        for param in self.vit_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Feature projection layers to match Detectron2's expected channels\n",
    "        self.feature_projections = nn.ModuleList([\n",
    "            nn.Conv2d(self.hidden_size, 256, 1) for _ in range(len(feature_layers))\n",
    "        ])\n",
    "        \n",
    "        # Initialize projection layers\n",
    "        for proj in self.feature_projections:\n",
    "            weight_init.c2_msra_fill(proj)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Extract multi-scale features from ViT.\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            List of feature maps at different scales\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Reshape to patches and add positional embedding\n",
    "        patches = self.vit_model.embeddings.patch_embeddings(x)  # (B, num_patches, hidden_size)\n",
    "        \n",
    "        # Add CLS token and positional embeddings\n",
    "        cls_tokens = self.vit_model.embeddings.cls_token.expand(B, -1, -1)\n",
    "        embeddings = torch.cat((cls_tokens, patches), dim=1)\n",
    "        embeddings = embeddings + self.vit_model.embeddings.position_embeddings\n",
    "        \n",
    "        # Apply transformer blocks and extract features at specified layers\n",
    "        hidden_states = embeddings\n",
    "        features = []\n",
    "        \n",
    "        for i, block in enumerate(self.vit_model.encoder.layer):\n",
    "            hidden_states = block(hidden_states)[0]\n",
    "            \n",
    "            # Extract features at specified layers (excluding CLS token)\n",
    "            if i + 1 in self.feature_layers:\n",
    "                # Remove CLS token and reshape to 2D\n",
    "                patch_features = hidden_states[:, 1:]  # Remove CLS token\n",
    "                \n",
    "                # Calculate patch grid size\n",
    "                num_patches_per_dim = int(np.sqrt(patch_features.shape[1]))\n",
    "                patch_features = patch_features.reshape(\n",
    "                    B, num_patches_per_dim, num_patches_per_dim, self.hidden_size\n",
    "                )\n",
    "                patch_features = patch_features.permute(0, 3, 1, 2)  # (B, C, H, W)\n",
    "                \n",
    "                # Project to expected channel size\n",
    "                projected_features = self.feature_projections[self.feature_layers.index(i + 1)](patch_features)\n",
    "                features.append(projected_features)\n",
    "        \n",
    "        return features\n",
    "\n",
    "class ViTBackbone(Backbone):\n",
    "    \"\"\"\n",
    "    ViT backbone for Detectron2.\n",
    "    Provides multi-scale features compatible with FPN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vit_model, feature_layers=[3, 6, 9, 12]):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(vit_model, feature_layers)\n",
    "        \n",
    "        # Define feature output shapes\n",
    "        # Assuming input size of 224x224 and patch size of 16\n",
    "        self._out_feature_channels = {\n",
    "            \"res3\": 256,  # 14x14 features\n",
    "            \"res4\": 256,  # 14x14 features  \n",
    "            \"res5\": 256,  # 14x14 features\n",
    "            \"res6\": 256,  # 14x14 features\n",
    "        }\n",
    "        \n",
    "        self._out_feature_strides = {\n",
    "            \"res3\": 16,  # 224/14 = 16\n",
    "            \"res4\": 16,\n",
    "            \"res5\": 16, \n",
    "            \"res6\": 16,\n",
    "        }\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through ViT backbone.\n",
    "        Args:\n",
    "            x: Input tensor of shape (B, C, H, W)\n",
    "        Returns:\n",
    "            Dict of feature maps at different scales\n",
    "        \"\"\"\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        return {\n",
    "            \"res3\": features[0],\n",
    "            \"res4\": features[1], \n",
    "            \"res5\": features[2],\n",
    "            \"res6\": features[3],\n",
    "        }\n",
    "    \n",
    "    def output_shape(self):\n",
    "        return {\n",
    "            name: ShapeSpec(\n",
    "                channels=self._out_feature_channels[name], \n",
    "                stride=self._out_feature_strides[name]\n",
    "            )\n",
    "            for name in self._out_feature_channels.keys()\n",
    "        }\n",
    "\n",
    "@BACKBONE_REGISTRY.register()\n",
    "def build_vit_backbone(cfg, input_shape: ShapeSpec):\n",
    "    \"\"\"\n",
    "    Build ViT backbone for Detectron2.\n",
    "    This function is called by Detectron2's model builder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if SSL backbone exists\n",
    "    ssl_path = config.SSL_BACKBONE_PATH\n",
    "    if not ssl_path.exists():\n",
    "        print(f\"Warning: SSL backbone not found at {ssl_path}\")\n",
    "        print(\"Using ImageNet pre-trained ViT instead\")\n",
    "        \n",
    "        # Use ImageNet pre-trained ViT\n",
    "        vit_model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        \n",
    "        # Convert to HuggingFace format for consistency\n",
    "        from transformers import ViTModel, ViTConfig\n",
    "        \n",
    "        # Create ViT config\n",
    "        vit_config = ViTConfig(\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            num_channels=3,\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072,\n",
    "        )\n",
    "        \n",
    "        # Create model and load weights\n",
    "        vit_model_hf = ViTModel(vit_config)\n",
    "        \n",
    "        # Map timm weights to HuggingFace format\n",
    "        timm_state = vit_model.state_dict()\n",
    "        hf_state = {}\n",
    "        \n",
    "        # Map embedding layer\n",
    "        hf_state['embeddings.patch_embeddings.projection.weight'] = timm_state['patch_embed.proj.weight']\n",
    "        hf_state['embeddings.patch_embeddings.projection.bias'] = timm_state['patch_embed.proj.bias']\n",
    "        hf_state['embeddings.cls_token'] = timm_state['cls_token']\n",
    "        hf_state['embeddings.position_embeddings'] = timm_state['pos_embed'][:, 1:]  # Remove class token\n",
    "        \n",
    "        # Map transformer layers\n",
    "        for i in range(12):\n",
    "            prefix = f'encoder.layer.{i}'\n",
    "            timm_prefix = f'blocks.{i}'\n",
    "            \n",
    "            # Attention\n",
    "            hf_state[f'{prefix}.attention.attention.query.weight'] = timm_state[f'{timm_prefix}.attn.qkv.weight'][:768]\n",
    "            hf_state[f'{prefix}.attention.attention.query.bias'] = timm_state[f'{timm_prefix}.attn.qkv.bias'][:768]\n",
    "            hf_state[f'{prefix}.attention.attention.key.weight'] = timm_state[f'{timm_prefix}.attn.qkv.weight'][768:1536]\n",
    "            hf_state[f'{prefix}.attention.attention.key.bias'] = timm_state[f'{timm_prefix}.attn.qkv.bias'][768:1536]\n",
    "            hf_state[f'{prefix}.attention.attention.value.weight'] = timm_state[f'{timm_prefix}.attn.qkv.weight'][1536:]\n",
    "            hf_state[f'{prefix}.attention.attention.value.bias'] = timm_state[f'{timm_prefix}.attn.qkv.bias'][1536:]\n",
    "            hf_state[f'{prefix}.attention.output.dense.weight'] = timm_state[f'{timm_prefix}.attn.proj.weight']\n",
    "            hf_state[f'{prefix}.attention.output.dense.bias'] = timm_state[f'{timm_prefix}.attn.proj.bias']\n",
    "            \n",
    "            # Layer norm\n",
    "            hf_state[f'{prefix}.layernorm_before.weight'] = timm_state[f'{timm_prefix}.norm1.weight']\n",
    "            hf_state[f'{prefix}.layernorm_before.bias'] = timm_state[f'{timm_prefix}.norm1.bias']\n",
    "            hf_state[f'{prefix}.layernorm_after.weight'] = timm_state[f'{timm_prefix}.norm2.weight']\n",
    "            hf_state[f'{prefix}.layernorm_after.bias'] = timm_state[f'{timm_prefix}.norm2.bias']\n",
    "            \n",
    "            # MLP\n",
    "            hf_state[f'{prefix}.intermediate.dense.weight'] = timm_state[f'{timm_prefix}.mlp.fc1.weight']\n",
    "            hf_state[f'{prefix}.intermediate.dense.bias'] = timm_state[f'{timm_prefix}.mlp.fc1.bias']\n",
    "            hf_state[f'{prefix}.output.dense.weight'] = timm_state[f'{timm_prefix}.mlp.fc2.weight']\n",
    "            hf_state[f'{prefix}.output.dense.bias'] = timm_state[f'{timm_prefix}.mlp.fc2.bias']\n",
    "        \n",
    "        # Load mapped weights\n",
    "        vit_model_hf.load_state_dict(hf_state)\n",
    "        vit_model = vit_model_hf\n",
    "        \n",
    "    else:\n",
    "        print(f\"Loading SSL backbone from {ssl_path}\")\n",
    "        \n",
    "        # Load SSL backbone\n",
    "        from transformers import ViTModel, ViTConfig\n",
    "        \n",
    "        # Load config\n",
    "        with open(config.MODELS_PATH / \"ssl_config.json\", 'r') as f:\n",
    "            config_dict = json.load(f)\n",
    "        \n",
    "        # Create ViT model\n",
    "        vit_config = ViTConfig(**config_dict)\n",
    "        vit_model = ViTModel(vit_config)\n",
    "        \n",
    "        # Load SSL weights\n",
    "        ssl_weights = torch.load(ssl_path, map_location='cpu')\n",
    "        vit_model.load_state_dict(ssl_weights)\n",
    "        \n",
    "        print(\"SSL backbone loaded successfully!\")\n",
    "    \n",
    "    # Create backbone\n",
    "    backbone = ViTBackbone(vit_model)\n",
    "    return backbone\n",
    "\n",
    "print(\"Custom ViT backbone implementation completed\")\n",
    "print(\"Backbone registered with Detectron2 as 'build_vit_backbone'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151df47b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_detectron2_config():\n",
    "    \"\"\"\n",
    "    Configure Detectron2 for Mask R-CNN training with ViT backbone.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get default config\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    # Start with a standard Mask R-CNN config\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    \n",
    "    # Replace backbone with our custom ViT backbone\n",
    "    cfg.MODEL.BACKBONE.NAME = \"build_vit_backbone\"\n",
    "    \n",
    "    # Input configuration\n",
    "    cfg.INPUT.MIN_SIZE_TRAIN = (512, 640, 704, 768)  # Multi-scale training\n",
    "    cfg.INPUT.MAX_SIZE_TRAIN = 1024\n",
    "    cfg.INPUT.MIN_SIZE_TEST = 512\n",
    "    cfg.INPUT.MAX_SIZE_TEST = 1024\n",
    "    \n",
    "    # Pixel normalization (ImageNet stats for ViT)\n",
    "    cfg.MODEL.PIXEL_MEAN = [123.675, 116.28, 103.53]\n",
    "    cfg.MODEL.PIXEL_STD = [58.395, 57.12, 57.375]\n",
    "    \n",
    "    # Model configuration\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Single class: lesion\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "    cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5\n",
    "    \n",
    "    # RPN configuration\n",
    "    cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN = 2000\n",
    "    cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 1000\n",
    "    cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 1000\n",
    "    cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 1000\n",
    "    \n",
    "    # Training configuration\n",
    "    cfg.DATASETS.TRAIN = (\"melanoma_train\",)\n",
    "    cfg.DATASETS.TEST = (\"melanoma_val\",)\n",
    "    \n",
    "    cfg.DATALOADER.NUM_WORKERS = 2  # Reduced for Windows compatibility\n",
    "    cfg.DATALOADER.ASPECT_RATIO_GROUPING = True\n",
    "    \n",
    "    # Solver configuration\n",
    "    cfg.SOLVER.IMS_PER_BATCH = config.BATCH_SIZE_DETECTRON\n",
    "    cfg.SOLVER.BASE_LR = config.LEARNING_RATE_DETECTRON\n",
    "    cfg.SOLVER.MAX_ITER = config.MAX_ITER_DETECTRON\n",
    "    cfg.SOLVER.STEPS = (3000, 4500)  # Learning rate decay steps\n",
    "    cfg.SOLVER.GAMMA = 0.1\n",
    "    cfg.SOLVER.WARMUP_ITERS = 500\n",
    "    cfg.SOLVER.WARMUP_FACTOR = 1.0 / 1000\n",
    "    cfg.SOLVER.WEIGHT_DECAY = 0.0001\n",
    "    cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
    "    \n",
    "    # Testing configuration\n",
    "    cfg.TEST.EVAL_PERIOD = 500\n",
    "    \n",
    "    # Output configuration\n",
    "    cfg.OUTPUT_DIR = str(config.MODELS_PATH / \"detectron2_output\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    return cfg\n",
    "\n",
    "# Setup configuration\n",
    "cfg = setup_detectron2_config()\n",
    "\n",
    "print(\"Detectron2 configuration setup completed\")\n",
    "print(f\"Output directory: {cfg.OUTPUT_DIR}\")\n",
    "print(f\"Training iterations: {cfg.SOLVER.MAX_ITER}\")\n",
    "print(f\"Batch size: {cfg.SOLVER.IMS_PER_BATCH}\")\n",
    "print(f\"Learning rate: {cfg.SOLVER.BASE_LR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6323b",
   "metadata": {},
   "source": [
    "### Custom Trainer with Augmentation Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumentationsMapper:\n",
    "    \"\"\"\n",
    "    Custom mapper that applies Albumentations augmentations to Detectron2 dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, augmentation_pipeline, is_train=True):\n",
    "        self.augmentation_pipeline = augmentation_pipeline\n",
    "        self.is_train = is_train\n",
    "    \n",
    "    def __call__(self, dataset_dict):\n",
    "        \"\"\"\n",
    "        Apply augmentations to a single dataset item.\n",
    "        Args:\n",
    "            dataset_dict: Dataset item in Detectron2 format\n",
    "        Returns:\n",
    "            Augmented dataset item\n",
    "        \"\"\"\n",
    "        dataset_dict = dataset_dict.copy()\n",
    "        \n",
    "        # Load image\n",
    "        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
    "        \n",
    "        # Convert BGR to RGB for albumentations\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Prepare masks and bboxes for augmentation\n",
    "        masks = []\n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        \n",
    "        for anno in dataset_dict[\"annotations\"]:\n",
    "            # Create mask from segmentation\n",
    "            mask = np.zeros((dataset_dict[\"height\"], dataset_dict[\"width\"]), dtype=np.uint8)\n",
    "            if \"segmentation\" in anno:\n",
    "                for seg in anno[\"segmentation\"]:\n",
    "                    if len(seg) >= 6:  # At least 3 points for a polygon\n",
    "                        pts = np.array(seg).reshape(-1, 2).astype(np.int32)\n",
    "                        cv2.fillPoly(mask, [pts], 255)\n",
    "            masks.append(mask)\n",
    "            \n",
    "            # Convert bbox format\n",
    "            bbox = anno[\"bbox\"]\n",
    "            if anno[\"bbox_mode\"] == BoxMode.XYXY_ABS:\n",
    "                bboxes.append(bbox)\n",
    "            else:  # XYWH_ABS\n",
    "                x1, y1, w, h = bbox\n",
    "                bboxes.append([x1, y1, x1 + w, y1 + h])\n",
    "            \n",
    "            class_labels.append(anno[\"category_id\"])\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.is_train and self.augmentation_pipeline:\n",
    "            try:\n",
    "                augmented = self.augmentation_pipeline(\n",
    "                    image=image,\n",
    "                    masks=masks,\n",
    "                    bboxes=bboxes,\n",
    "                    class_labels=class_labels\n",
    "                )\n",
    "                \n",
    "                image = augmented[\"image\"]\n",
    "                masks = augmented[\"masks\"]\n",
    "                bboxes = augmented[\"bboxes\"]\n",
    "                class_labels = augmented[\"class_labels\"]\n",
    "                \n",
    "                # Convert RGB back to BGR for Detectron2\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Augmentation error: {e}\")\n",
    "                # Use original data if augmentation fails\n",
    "                pass\n",
    "        \n",
    "        # Convert image to tensor and normalize\n",
    "        image = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
    "        \n",
    "        # Update dataset dict\n",
    "        dataset_dict[\"image\"] = image\n",
    "        dataset_dict[\"height\"] = image.shape[1]\n",
    "        dataset_dict[\"width\"] = image.shape[2]\n",
    "        \n",
    "        # Convert masks and bboxes back to Detectron2 format\n",
    "        instances = []\n",
    "        for i, (mask, bbox, class_id) in enumerate(zip(masks, bboxes, class_labels)):\n",
    "            # Create instance from mask\n",
    "            mask = torch.as_tensor(mask.astype(\"uint8\"))\n",
    "            \n",
    "            # Convert bbox to XYXY format\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            bbox = torch.as_tensor([x1, y1, x2, y2], dtype=torch.float32)\n",
    "            \n",
    "            # Create instance\n",
    "            instance = {\n",
    "                \"bbox\": bbox,\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": mask,\n",
    "                \"category_id\": class_id,\n",
    "            }\n",
    "            instances.append(instance)\n",
    "        \n",
    "        dataset_dict[\"annotations\"] = instances\n",
    "        \n",
    "        return dataset_dict\n",
    "\n",
    "class AugmentationTrainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    Custom trainer that integrates Albumentations with Detectron2.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        \n",
    "        # Get augmentation pipeline\n",
    "        self.train_transform, _ = get_augmentation_pipeline()\n",
    "    \n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        \"\"\"\n",
    "        Build training data loader with augmentation.\n",
    "        \"\"\"\n",
    "        mapper = AlbumentationsMapper(\n",
    "            augmentation_pipeline=cls.get_augmentation_pipeline(),\n",
    "            is_train=True\n",
    "        )\n",
    "        \n",
    "        return build_detection_train_loader(cfg, mapper=mapper)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_augmentation_pipeline(cls):\n",
    "        \"\"\"Get augmentation pipeline for training\"\"\"\n",
    "        train_transform, _ = get_augmentation_pipeline()\n",
    "        return train_transform\n",
    "    \n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        \"\"\"\n",
    "        Build test data loader without augmentation.\n",
    "        \"\"\"\n",
    "        mapper = AlbumentationsMapper(\n",
    "            augmentation_pipeline=None,\n",
    "            is_train=False\n",
    "        )\n",
    "        \n",
    "        return build_detection_test_loader(cfg, dataset_name, mapper=mapper)\n",
    "\n",
    "# Test trainer initialization\n",
    "try:\n",
    "    trainer = AugmentationTrainer(cfg)\n",
    "    print(\"Custom trainer initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing trainer: {e}\")\n",
    "    print(\"This is expected if datasets are not available yet\")\n",
    "\n",
    "print(\"Custom trainer with augmentation integration implemented\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34914eab",
   "metadata": {},
   "source": [
    "### Training Execution and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0124d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mask_rcnn_training():\n",
    "    \"\"\"\n",
    "    Execute Mask R-CNN training with custom ViT backbone.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if final model already exists\n",
    "    if config.FINAL_MODEL_PATH.exists():\n",
    "        print(f\"Final model already exists at {config.FINAL_MODEL_PATH}\")\n",
    "        print(\"Skipping training. To retrain, delete the existing file first.\")\n",
    "        return\n",
    "    \n",
    "    # Check if datasets are available\n",
    "    train_dicts = get_melanoma_dicts(str(config.TRAIN_IMAGES_PATH), str(config.TRAIN_MASKS_PATH))\n",
    "    val_dicts = get_melanoma_dicts(str(config.VAL_IMAGES_PATH), str(config.VAL_MASKS_PATH))\n",
    "    \n",
    "    if not train_dicts:\n",
    "        print(f\"No training data found in {config.TRAIN_IMAGES_PATH}\")\n",
    "        print(\"Please place labeled images and masks in the data/train/ directory\")\n",
    "        return\n",
    "    \n",
    "    if not val_dicts:\n",
    "        print(f\"No validation data found in {config.VAL_IMAGES_PATH}\")\n",
    "        print(\"Please place labeled images and masks in the data/val/ directory\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Starting Mask R-CNN training...\")\n",
    "    print(f\"Training samples: {len(train_dicts)}\")\n",
    "    print(f\"Validation samples: {len(val_dicts)}\")\n",
    "    \n",
    "    try:\n",
    "        # Initialize trainer\n",
    "        trainer = AugmentationTrainer(cfg)\n",
    "        \n",
    "        # Start training\n",
    "        trainer.resume_or_load(resume=False)\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "        \n",
    "        # Save final model\n",
    "        save_final_model(trainer)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def save_final_model(trainer):\n",
    "    \"\"\"Save the final trained model\"\"\"\n",
    "    print(\"Saving final model...\")\n",
    "    \n",
    "    # Get the best checkpoint\n",
    "    checkpointer = DetectionCheckpointer(trainer.model, save_dir=cfg.OUTPUT_DIR)\n",
    "    checkpointer.save(\"final_model\")\n",
    "    \n",
    "    # Copy to our models directory\n",
    "    import shutil\n",
    "    final_checkpoint = Path(cfg.OUTPUT_DIR) / \"final_model.pth\"\n",
    "    if final_checkpoint.exists():\n",
    "        shutil.copy2(final_checkpoint, config.FINAL_MODEL_PATH)\n",
    "        print(f\"Final model saved to {config.FINAL_MODEL_PATH}\")\n",
    "    \n",
    "    # Save config\n",
    "    with open(config.CONFIG_PATH, 'w') as f:\n",
    "        f.write(cfg.dump())\n",
    "    print(f\"Config saved to {config.CONFIG_PATH}\")\n",
    "\n",
    "def evaluate_model():\n",
    "    \"\"\"Evaluate the trained model on validation set\"\"\"\n",
    "    \n",
    "    if not config.FINAL_MODEL_PATH.exists():\n",
    "        print(\"No trained model found. Please train the model first.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Evaluating model on validation set...\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = build_model(cfg)\n",
    "        checkpointer = DetectionCheckpointer(model)\n",
    "        checkpointer.load(str(config.FINAL_MODEL_PATH))\n",
    "        model.eval()\n",
    "        \n",
    "        # Get validation dataset\n",
    "        val_dataset = DatasetCatalog.get(\"melanoma_val\")\n",
    "        metadata = MetadataCatalog.get(\"melanoma_val\")\n",
    "        \n",
    "        # Run evaluation\n",
    "        from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "        from detectron2.data import build_detection_test_loader\n",
    "        \n",
    "        evaluator = COCOEvaluator(\"melanoma_val\", output_dir=cfg.OUTPUT_DIR)\n",
    "        val_loader = build_detection_test_loader(cfg, \"melanoma_val\")\n",
    "        \n",
    "        results = inference_on_dataset(model, val_loader, evaluator)\n",
    "        \n",
    "        print(\"Evaluation results:\")\n",
    "        print(results)\n",
    "        \n",
    "        # Visualize some predictions\n",
    "        visualize_predictions(model, val_dataset[:4], metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def visualize_predictions(model, dataset_dicts, metadata, confidence_threshold=0.5):\n",
    "    \"\"\"Visualize model predictions on sample images\"\"\"\n",
    "    \n",
    "    print(\"Visualizing predictions...\")\n",
    "    \n",
    "    for d in dataset_dicts:\n",
    "        # Load image\n",
    "        img = cv2.imread(d[\"file_name\"])\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model([{\"image\": torch.as_tensor(img.transpose(2, 0, 1).astype(\"float32\"))}])[0]\n",
    "        \n",
    "        # Filter predictions by confidence\n",
    "        instances = outputs[\"instances\"]\n",
    "        instances = instances[instances.scores > confidence_threshold]\n",
    "        \n",
    "        # Visualize\n",
    "        v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.2)\n",
    "        out = v.draw_instance_predictions(instances.to(\"cpu\"))\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(out.get_image())\n",
    "        plt.title(f\"Predictions (confidence > {confidence_threshold})\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Training and evaluation functions\n",
    "print(\"Training and evaluation functions defined\")\n",
    "print(\"Call run_mask_rcnn_training() to start training\")\n",
    "print(\"Call evaluate_model() to evaluate trained model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e9eb1",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### Complete Training Pipeline\n",
    "\n",
    "1. **Prepare Data**:\n",
    "   - Place unlabeled images in `data/images/` for SSL training\n",
    "   - Place labeled images in `data/train/images/` and masks in `data/train/masks/`\n",
    "   - Place validation images in `data/val/images/` and masks in `data/val/masks/`\n",
    "\n",
    "2. **Run SSL Pre-training** (Optional):\n",
    "   ```python\n",
    "   run_ssl_training()  # Train MAE backbone on unlabeled data\n",
    "   ```\n",
    "\n",
    "3. **Run Mask R-CNN Training**:\n",
    "   ```python\n",
    "   run_mask_rcnn_training()  # Train Mask R-CNN with ViT backbone\n",
    "   ```\n",
    "\n",
    "4. **Evaluate Model**:\n",
    "   ```python\n",
    "   evaluate_model()  # Evaluate on validation set\n",
    "   ```\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "- **SSL Backbone**: `models/ssl_vit_backbone.pth`\n",
    "- **Final Model**: `models/final_lesion_segmenter.pth`\n",
    "- **Config**: `models/config.yaml`\n",
    "\n",
    "The trained model can be used with the standalone `predict.py` script for inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ed1cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609310a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
